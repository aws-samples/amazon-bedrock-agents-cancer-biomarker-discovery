AWSTemplateFormatVersion: '2010-09-09'
Description: Creates a Bedrock Agent and action group

Parameters:
  BedrockModelId:
    Type: String
    Description: The ID of the Foundation Model to use for the Agent
    Default: anthropic.claude-3-5-sonnet-20240620-v1:0
  EnvironmentName:
    Type: String
    Description: The name of the agent environment, used to differentiate agent application. Must be lowercase, contain one number, and be no more than 5 characters long.
    Default: env1
    MaxLength: 5
    AllowedPattern: ^[a-z]{1,4}[0-9]$
    ConstraintDescription: Must be lowercase, contain one number at the end, and be no more than 5 characters long.
  GitRepoURL:
    Type: String
    Default: 'https://github.com/berilunay/amazon-bedrock-agents-cancer-biomarker-discovery.git'
    Description: Git repository URL where the code files are stored
  ImageTag:
    Type: String
    Default: latest
    Description: Tag for the Docker image
  VectorStoreName:
    Type: String
    Default: nmcicollection
    Description: Name of the vector store/collection
  IndexName:
    Type: String
    Default: vector-index
    Description: Name of the vector index to be created
  GitBranch:
    Type: String
    Description: 'The github branch to clone, change only for dev testing'
    Default: 'main'


Mappings:
  RegionMap:
    us-east-1:
      PandasLayer: 'arn:aws:lambda:us-east-1:336392948345:layer:AWSSDKPandas-Python312:12'
    us-east-2:
      PandasLayer: 'arn:aws:lambda:us-east-2:336392948345:layer:AWSSDKPandas-Python312:12'
    us-west-1:
      PandasLayer: 'arn:aws:lambda:us-west-1:336392948345:layer:AWSSDKPandas-Python312:12'
    us-west-2:
      PandasLayer: 'arn:aws:lambda:us-west-2:336392948345:layer:AWSSDKPandas-Python312:12'



Resources:
  # Create ECR repository
  ECRRepository:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: lifelines-lambda-sample

  EnsureECRImagePushed:
    Type: Custom::EnsureECRImagePushed
    DependsOn: 
      - TriggerBuildCustomResource
      - TriggerImagingDockerBuild
    Properties:
      ServiceToken: !GetAtt TriggerBuildLambda.Arn
      ECRRepository: !Ref ECRRepository
      ImageTag: !Ref ImageTag
  
  EnsureImagingDockerImagePushed:
    Type: Custom::EnsureECRImagePushed
    DependsOn: 
      - TriggerBuildCustomResource
      - TriggerImagingDockerBuild
    Properties:
      ServiceToken: !GetAtt TriggerBuildLambda.Arn
      ECRRepository: !Ref ImagingECRRepository
      ImageTag: !Ref ImageTag
      

  CleanupLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt CleanupLambdaRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          
          def delete_ecr_images(repository_name):
              ecr = boto3.client('ecr')
              paginator = ecr.get_paginator('list_images')
              try:
                  for page in paginator.paginate(repositoryName=repository_name):
                      if 'imageIds' in page:
                          ecr.batch_delete_image(
                              repositoryName=repository_name,
                              imageIds=page['imageIds']
                          )
                  print(f"All images deleted from repository: {repository_name}")
              except Exception as e:
                  print(f"Error deleting images from repository {repository_name}: {str(e)}")
                  raise

          def handler(event, context):
              if event['RequestType'] == 'Delete':
                  try:
                      # Clean up ECR
                      ecr = boto3.client('ecr')
                      for repo_name in [event['ResourceProperties']['ECRRepositoryName'], 'medical-image-processing-smstudio']:
                          try:
                              delete_ecr_images(repo_name)
                              ecr.delete_repository(repositoryName=repo_name)
                              print(f"Repository {repo_name} deleted successfully")
                          except ecr.exceptions.RepositoryNotFoundException:
                              print(f"Repository {repo_name} not found, skipping deletion")
                          except Exception as e:
                              print(f"Error deleting repository {repo_name}: {str(e)}")
                              # Continue with other cleanup tasks even if ECR deletion fails
                
                      # Clean up S3
                      s3 = boto3.resource('s3')
                      bucket = s3.Bucket(event['ResourceProperties']['S3BucketName'])
                      bucket.objects.all().delete()
                      logbucket = s3.Bucket(event['ResourceProperties']['LogBucketName'])
                      logbucket.objects.all().delete()
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                  except Exception as e:
                      print(e)
                      cfnresponse.send(event, context, cfnresponse.FAILED, {})
              else:
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
      Runtime: python3.12
      Timeout: 300

  CleanupLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: CleanupPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:ListBucket'
                  - 's3:DeleteObject'
                Resource: 
                  - !Sub 'arn:aws:s3:::${S3Bucket}'
                  - !Sub 'arn:aws:s3:::${S3Bucket}/*'
                  - !Sub 'arn:aws:s3:::${LogBucket}'
                  - !Sub 'arn:aws:s3:::${LogBucket}/*'
              - Effect: Allow
                Action:
                  - 'ecr:ListImages'
                  - 'ecr:BatchDeleteImage'
                  - 'ecr:DeleteRepository'
                  - 'ecr:DescribeRepositories'
                Resource: 
                  - !Sub 'arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/${ECRRepository}'
                  - !Sub 'arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/medical-image-processing-smstudio'
              - Effect: Allow
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*'

  CleanupCustomResource:
    Type: Custom::Cleanup
    DependsOn: 
      - S3Bucket
      - ECRRepository
    Properties:
      ServiceToken: !GetAtt CleanupLambdaFunction.Arn
      ECRRepositoryName: !Ref ECRRepository
      S3BucketName: !Ref S3Bucket
      LogBucketName: !Ref LogBucket
  
  
  S3Bucket: 
     Type: AWS::S3::Bucket
     Properties:
       BucketName: !Sub '${EnvironmentName}-${AWS::AccountId}-${AWS::Region}-agent-build-bucket'
       BucketEncryption:
         ServerSideEncryptionConfiguration:
           - ServerSideEncryptionByDefault:
               SSEAlgorithm: AES256
       LoggingConfiguration:
         DestinationBucketName: !Ref LogBucket
         LogFilePrefix: 'access-logs/'

  S3BucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref S3Bucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: ForceSSLOnlyAccess
            Effect: Deny
            Principal: '*'
            Action: 's3:*'
            Resource: 
              - !Sub '${S3Bucket.Arn}'
              - !Sub '${S3Bucket.Arn}/*'
            Condition:
              Bool:
                'aws:SecureTransport': false

  LogBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${EnvironmentName}-${AWS::AccountId}-${AWS::Region}-log-bucket'
  
  LogBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref LogBucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: AllowLogDeliveryWrite
            Effect: Allow
            Principal:
              Service: logging.s3.amazonaws.com
            Action:
              - s3:PutObject
            Resource: !Sub '${LogBucket.Arn}/*'

  # Create CloudWatch Log Group
  CodeBuildLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/codebuild/${AWS::StackName}-DockerPushProject'
      RetentionInDays: 14
  
  
  #Gather Assets for pubmed lambda function, knowledgebase and survival data processing lambda
  DataRetrievalLogic:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: DataRetrievalLogic
      ServiceRole: !GetAtt CodeBuildRole.Arn
      Artifacts:
        Type: NO_ARTIFACTS
      LogsConfig:
        CloudWatchLogs:
          Status: ENABLED
          GroupName: !Ref CodeBuildLogGroup
      Environment:
        Type: LINUX_CONTAINER
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/amazonlinux2-x86_64-standard:5.0
        PrivilegedMode: true
      Source:
        Type: GITHUB
        Location: !Ref GitRepoURL
        BuildSpec: !Sub |
          version: 0.2
          phases:
            pre_build:
              commands:
                - echo "downloading Ncbi article for KB ingestion"
                - yum install -y wget
               
            build:
              commands:
                - echo "Starting build phase"
                - echo "Downloading NCBI article..."
                - wget --user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36" https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5749594/pdf/radiol.2017161845.pdf -O ncbi_article.pdf
                - echo "Uploading NCBI article to S3..."
                - aws s3 cp ncbi_article.pdf s3://${S3Bucket}/ncbi_article.pdf
                - echo "NCBI article uploaded to S3"
                - echo "Cloning Git repository..."
                - git clone -b ${GitBranch} --single-branch ${GitRepoURL} repo
                - echo "Zipping Lambda function..."
                - cd ActionGroups/matplotbarchartlambda
                - echo "Creating list of items to zip..."
                - items_to_zip=$(ls -A | tr '\n' ' ')
                - zip -r matplotbarchartlambda.zip $items_to_zip
                - cd ../..
                - echo "Moving zip file to project root..."
                - mv ActionGroups/matplotbarchartlambda/matplotbarchartlambda.zip .
                - aws s3 cp matplotbarchartlambda.zip s3://${S3Bucket}/matplotbarchartlambda.zip
                - cd repo/ActionGroups/pubmed-lambda-function
                - echo "Creating list of items to zip..."
                - items_to_zip=$(ls -A | tr '\n' ' ')
                - echo "Creating zip pubmed lambda file..."
                - zip -r pubmed-lambda-function.zip $items_to_zip
                - echo "Lambda function zipped"
                - cd ../..
                - echo "Moving zip file to project root..."
                - mv ActionGroups/pubmed-lambda-function/pubmed-lambda-function.zip .
                - aws s3 cp pubmed-lambda-function.zip s3://${S3Bucket}/pubmed-lambda-function.zip
                - cd ActionGroups/querydatabaselambda
                - echo "Creating list of items to zip..."
                - items_to_zip=$(ls -A | tr '\n' ' ')
                - zip -r querydatabaselambda.zip $items_to_zip
                - cd ../..
                - echo "Moving zip file to project root..."
                - mv ActionGroups/querydatabaselambda/querydatabaselambda.zip .
                - aws s3 cp querydatabaselambda.zip s3://${S3Bucket}/querydatabaselambda.zip
                - cd ActionGroups/survivaldataprocessinglambda
                - echo "Creating list of items to zip..."
                - items_to_zip=$(ls -A | tr '\n' ' ')
                - zip -r survivaldataprocessinglambda.zip $items_to_zip
                - cd ../..
                - echo "Moving zip file to project root..."
                - mv ActionGroups/survivaldataprocessinglambda/survivaldataprocessinglambda.zip .
                - aws s3 cp survivaldataprocessinglambda.zip s3://${S3Bucket}/survivaldataprocessinglambda.zip
                - echo "Build phase completed"
                - echo "lambda layer preperation phase starts"
                - mkdir -p python
                - pip install -r lambdalayers/requirements.txt -t python
                - zip -r9 lambda-layer.zip python
                - aws s3 cp lambda-layer.zip s3://${S3Bucket}/lambda-layer.zip

      TimeoutInMinutes: 10


  # Log into ECR and push scientific-plots-with-lifelines Docker image
  DockerPushLogic:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: DockerPushProject
      ServiceRole: !GetAtt CodeBuildRole.Arn
      Artifacts:
        Type: NO_ARTIFACTS
      LogsConfig:
        CloudWatchLogs:
          Status: ENABLED
          GroupName: !Ref CodeBuildLogGroup
      Environment:
        Type: LINUX_CONTAINER
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/amazonlinux2-x86_64-standard:5.0
        PrivilegedMode: true
      Source:
        Type: GITHUB
        Location: !Ref GitRepoURL
        BuildSpec: !Sub |
          version: 0.2
          phases:
            pre_build:
              commands:
                - echo "Starting pre_build phase"
                - echo "Logging into Amazon ECR..."
                - aws ecr get-login-password --region ${AWS::Region} | docker login --username AWS --password-stdin ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com
                - echo "ECR login complete"
            build:
              commands:
                - echo "Starting build phase"
                - echo "Cloning Git repository..."
                - git clone -b ${GitBranch} --single-branch ${GitRepoURL} repo
                - cd repo/ActionGroups/scientific-plots-with-lifelines
                - echo "Building Docker image..."
                - docker build -t lifelines-python3.12-v2 .
                - echo "Tagging Docker image..."
                - docker tag lifelines-python3.12-v2:latest ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ECRRepository}:${ImageTag}
                - echo "Docker image tagged"
            post_build:
              commands:
                - echo "Starting post_build phase"
                - echo "Pushing Docker image to ECR..."
                - docker push ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ECRRepository}:${ImageTag}
                - echo "Docker image pushed to ECR"
                - echo "Displaying Docker images"
                - docker images
                - echo "Displaying ECR repository contents"
                - aws ecr list-images --repository-name ${ECRRepository}
      TimeoutInMinutes: 10

  ImagingECRRepository:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: medical-image-processing-smstudio

  ImagingDockerBuildProject:
    Type: AWS::CodeBuild::Project
    Properties:
      Artifacts:
        Type: NO_ARTIFACTS
      Environment:
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/amazonlinux2-x86_64-standard:3.0
        Type: LINUX_CONTAINER
        PrivilegedMode: true
      ServiceRole: !GetAtt CodeBuildRole.Arn
      Source:
        Type: GITHUB
        Location: !Ref GitRepoURL
        BuildSpec: !Sub |
          version: 0.2
          phases:
            pre_build:
              commands:
                - echo Logging in to Amazon ECR...
                - aws ecr get-login-password --region ${AWS::Region} | docker login --username AWS --password-stdin ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com
                - echo Cloning the repository...
                - git clone -b ${GitBranch} --single-branch ${GitRepoURL} repo
                - cd repo/ActionGroups/imaging-biomarker 
                - echo Checking for required files...
                - ls -la
                - if [ ! -f requirements.txt ] || [ ! -f dcm2nifti_processing.py ] || [ ! -f radiomics_utils.py ]; then echo "Missing required files"; exit 1; fi
                - zip -r Imaginglambdafunction.zip dummy_lambda.py
                - echo Copying lambda function 
                - aws s3 cp Imaginglambdafunction.zip s3://${S3Bucket}/Imaginglambdafunction.zip
               

            build:
              commands:
                - echo Build started on `date`
                - echo Building the Docker image...
                - docker build -t ${ImagingECRRepository}:${ImageTag} .
                - docker tag ${ImagingECRRepository}:${ImageTag} ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ImagingECRRepository}:${ImageTag}
            post_build:
              commands:
                - echo Build completed on `date`
                - echo Pushing the Docker image...
                - docker push ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ImagingECRRepository}:${ImageTag}
      SourceVersion: main
  
  ImagingStateMachine:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      RoleArn: !GetAtt StepFunctionsExecutionRole.Arn
      DefinitionString: !Sub
        - |
          {
            "StartAt": "iterate_over_subjects",
            "States": {
              "iterate_over_subjects": {
                "ItemsPath": "$.Subject",
                "MaxConcurrency": 50,
                "Type": "Map",
                "Next": "Finish",
                "Iterator": {
                  "StartAt": "DICOM/NIfTI Conversion and Radiomic Feature Extraction",
                  "States": {
                    "Fallback": {
                      "Type": "Pass",
                      "Result": "This iteration failed for some reason",
                      "End": true
                    },
                    "DICOM/NIfTI Conversion and Radiomic Feature Extraction": {
                      "Type": "Task",
                      "OutputPath": "$.ProcessingJobArn",
                      "Resource": "arn:aws:states:::sagemaker:createProcessingJob.sync",
                      "Retry": [
                        {
                          "ErrorEquals": [
                            "SageMaker.AmazonSageMakerException"
                          ],
                          "IntervalSeconds": 15,
                          "MaxAttempts": 8,
                          "BackoffRate": 1.5
                        }
                      ],
                      "Catch": [
                        {
                          "ErrorEquals": [
                            "States.TaskFailed"
                          ],
                          "Next": "Fallback"
                        }
                      ],
                      "Parameters": {
                        "ProcessingJobName.$": "States.Format('{}-{}', $$.Execution.Input['PreprocessingJobName'], $)",
                        "ProcessingInputs": [
                          {
                            "InputName": "DICOM",
                            "AppManaged": false,
                            "S3Input": {
                              "S3Uri.$": "States.Format('s3://sagemaker-solutions-prod-${AWS::Region}/sagemaker-lung-cancer-survival-prediction/1.1.0/data/nsclc_radiogenomics/{}' , $)", 
                              "LocalPath": "/opt/ml/processing/input",
                              "S3DataType": "S3Prefix",
                              "S3InputMode": "File",
                              "S3DataDistributionType": "FullyReplicated",
                              "S3CompressionType": "None"
                            }
                          }
                        ],
                        "ProcessingOutputConfig": {
                          "Outputs": [
                            {
                              "OutputName": "CT-Nifti",
                              "AppManaged": false,
                              "S3Output": {
                                "S3Uri": "${S3Bucket}/nsclc_radiogenomics/CT-Nifti",
                                "LocalPath": "/opt/ml/processing/output/CT-Nifti",
                                "S3UploadMode": "EndOfJob"
                              }
                            },
                            {
                              "OutputName": "CT-SEG",
                              "AppManaged": false,
                              "S3Output": {
                                "S3Uri": "${S3Bucket}/nsclc_radiogenomics/CT-SEG",
                                "LocalPath": "/opt/ml/processing/output/CT-SEG",
                                "S3UploadMode": "EndOfJob"
                              }
                            },
                            {
                              "OutputName": "PNG",
                              "AppManaged": false,
                              "S3Output": {
                                "S3Uri": "${S3Bucket}/nsclc_radiogenomics/PNG",
                                "LocalPath": "/opt/ml/processing/output/PNG",
                                "S3UploadMode": "EndOfJob"
                              }
                            },
                            {
                              "OutputName": "CSV",
                              "AppManaged": false,
                              "S3Output": {
                                "S3Uri": "${S3Bucket}/nsclc_radiogenomics/CSV",
                                "LocalPath": "/opt/ml/processing/output/CSV",
                                "S3UploadMode": "EndOfJob"
                              }
                            }
                          ]
                        },
                        "AppSpecification": {
                          "ImageUri": "${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ImagingECRRepository}:${ImageTag}",
                          "ContainerArguments.$": "States.Array('--subject', $)",
                          "ContainerEntrypoint": [
                            "python3",
                            "/opt/dcm2nifti_processing.py"
                          ]
                        },
                        "RoleArn": "${SageMakerExecutionRoleArn}",
                        "ProcessingResources": {
                          "ClusterConfig": {
                            "InstanceCount": 1,
                            "InstanceType": "ml.m5.xlarge",
                            "VolumeSizeInGB": 5
                          }
                        }
                      },
                      "End": true
                    }
                  }
                }
              },
              "Finish": {
                "Type": "Succeed"
              }
            }
          }

        - {
            S3Bucket: !Sub "s3://${S3Bucket}",
            SageMakerExecutionRoleArn: !GetAtt SageMakerExecutionRole.Arn
          }


  # IAM role for CodeBuild
  CodeBuildRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: codebuild.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPowerUser'
        - 'arn:aws:iam::aws:policy/CloudWatchLogsFullAccess'
      Policies:
        - PolicyName: CodeBuildBasePolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Resource: '*'
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
              - Effect: Allow
                Resource: 
                  - !Sub 'arn:aws:s3:::${AWS::AccountId}-codepipeline-${AWS::Region}'
                  - !Sub 'arn:aws:s3:::${AWS::AccountId}-codepipeline-${AWS::Region}/*'
                Action:
                  - 's3:GetObject'
                  - 's3:GetObjectVersion'
                  - 's3:GetBucketVersioning'
                  - 's3:PutObject'
              - Effect: Allow
                Resource: '*'
                Action:
                  - 'ecr:GetAuthorizationToken'
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                  - s3:ListBucket
                Resource: 
                  - !GetAtt S3Bucket.Arn
                  - !Sub "${S3Bucket.Arn}/*"
        - PolicyName: ECRAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - ecr:GetAuthorizationToken
                  - ecr:BatchCheckLayerAvailability
                  - ecr:GetDownloadUrlForLayer
                  - ecr:GetRepositoryPolicy
                  - ecr:DescribeRepositories
                  - ecr:ListImages
                  - ecr:DescribeImages
                  - ecr:BatchGetImage
                  - ecr:InitiateLayerUpload
                  - ecr:UploadLayerPart
                  - ecr:CompleteLayerUpload
                  - ecr:PutImage
                Resource: '*'
  
  StepFunctionsExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: states.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: StepFunctionsExecutionPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'sagemaker:CreateProcessingJob'
                  - 'sagemaker:DescribeProcessingJob'
                  - 'sagemaker:StopProcessingJob'
                  - 'sagemaker:ListTags'
                  - 'sagemaker:AddTags'
                Resource: '*'
              - Effect: Allow
                Action:
                  - 'iam:PassRole'
                Resource: !GetAtt SageMakerExecutionRole.Arn
              - Effect: Allow
                Action:
                  - 'events:PutTargets'
                  - 'events:PutRule'
                  - 'events:DescribeRule'
                Resource: '*'
              - Effect: Allow
                Action:
                  - 'logs:CreateLogDelivery'
                  - 'logs:GetLogDelivery'
                  - 'logs:UpdateLogDelivery'
                  - 'logs:DeleteLogDelivery'
                  - 'logs:ListLogDeliveries'
                  - 'logs:PutResourcePolicy'
                  - 'logs:DescribeResourcePolicies'
                  - 'logs:DescribeLogGroups'
                Resource: '*'
  
  SageMakerExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: sagemaker.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/AmazonSageMakerFullAccess'
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:PutObject'
                  - 's3:ListBucket'
                Resource: 
                  - !Sub 'arn:aws:s3:::${S3Bucket}'
                  - !Sub 'arn:aws:s3:::${S3Bucket}/*'
                  - !Sub 'arn:aws:s3:::sagemaker-solutions-prod-${AWS::Region}/sagemaker-lung-cancer-survival-prediction/*'


  TriggerBuildLambda:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt TriggerBuildLambdaRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import time

          def handler(event, context):
              if event['RequestType'] in ['Create', 'Update']:
                  try:
                      codebuild = boto3.client('codebuild')
                      ecr = boto3.client('ecr')
                      
                      if 'ProjectName' in event['ResourceProperties']:
                          # This is for TriggerBuildCustomResource
                          project_name = event['ResourceProperties']['ProjectName']
                          response = codebuild.start_build(projectName=project_name)
                          build_id = response['build']['id']
                          print(f"Build started: {build_id}")
                          
                          # Wait for build to complete
                          while True:
                              build_status = codebuild.batch_get_builds(ids=[build_id])['builds'][0]['buildStatus']
                              if build_status == 'SUCCEEDED':
                                  print("Build completed successfully")
                                  break
                              elif build_status in ['FAILED', 'STOPPED', 'TIMED_OUT']:
                                  print(f"Build failed with status: {build_status}")
                                  cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": f"Build failed with status: {build_status}"})
                                  return
                              time.sleep(10)  # Wait for 10 seconds before checking again
                          
                          cfnresponse.send(event, context, cfnresponse.SUCCESS, {"BuildId": build_id})
                          
                      elif 'ECRRepository' in event['ResourceProperties']:
                          # This is for EnsureECRImagePushed
                          repository_name = event['ResourceProperties']['ECRRepository']
                          image_tag = event['ResourceProperties']['ImageTag']
                          
                          # Wait for image to be available in ECR
                          max_attempts = 30  # Maximum number of attempts
                          for attempt in range(max_attempts):
                              try:
                                  ecr.describe_images(repositoryName=repository_name, imageIds=[{'imageTag': image_tag}])
                                  print(f"Image {repository_name}:{image_tag} exists in ECR")
                                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                                  return
                              except ecr.exceptions.ImageNotFoundException:
                                  if attempt == max_attempts - 1:
                                      print(f"Image {repository_name}:{image_tag} not found in ECR after {max_attempts} attempts")
                                      cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": "Image not found in ECR after maximum attempts"})
                                      return
                                  time.sleep(10)  # Wait for 10 seconds before trying again
                      else:
                          cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": "Invalid ResourceProperties"})
                  except Exception as e:
                      print(f"Error: {str(e)}")
                      cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": str(e)})
              else:
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
      Runtime: python3.8
      Timeout: 900

  TriggerBuildLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: CodeBuildStartBuildPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: 
                  - codebuild:StartBuild
                  - codebuild:BatchGetBuilds
                Resource: 
                  - !GetAtt DockerPushLogic.Arn
                  - !GetAtt DataRetrievalLogic.Arn
                  - !Sub arn:aws:codebuild:${AWS::Region}:${AWS::AccountId}:project/ImagingDockerBuildProject-*
        - PolicyName: CloudWatchLogsPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: arn:aws:logs:*:*:*
        - PolicyName: ECRDescribeImagesPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: ecr:DescribeImages
                Resource: '*'
        - PolicyName: PassRoleToCodeBuild
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - iam:PassRole
                Resource: !GetAtt CodeBuildRole.Arn
  
  TriggerBuildCustomResource:
    Type: Custom::TriggerBuild
    DependsOn: DockerPushLogic
    Properties:
      ServiceToken: !GetAtt TriggerBuildLambda.Arn
      ProjectName: !Ref DockerPushLogic
  
  TriggerImagingDockerBuild:
    Type: Custom::TriggerBuild
    DependsOn: ImagingDockerBuildProject
    Properties:
      ServiceToken: !GetAtt TriggerBuildLambda.Arn
      ProjectName: !Ref ImagingDockerBuildProject
  
  TriggerDataRetrievalLogic:
    Type: Custom::TriggerBuild
    DependsOn: DataRetrievalLogic
    Properties:
      ServiceToken: !GetAtt TriggerBuildLambda.Arn
      ProjectName: !Ref DataRetrievalLogic
  

  AgentResource:
    Type: AWS::Bedrock::Agent
    DependsOn:
      - BedrockKnowledgeBase
      - KnowledgeBaseDataSource
    Properties:
      AgentName: !Sub biomarker-agent-${EnvironmentName}
      AgentResourceRoleArn: !GetAtt AgentRole.Arn
      AutoPrepare : True
      FoundationModel: !Ref BedrockModelId
      Instruction: |
        You are a medical research assistant AI specialized in generating SQL queries for a database containing medical biomarker information. Your primary task is to interpret user queries, generate appropriate SQL queries, and provide relevant medical insights based on the data. Use only the appropriate tools as required by the specific question. Follow these instructions carefully:

        1. Available tools and their functions:
          - /getschema: Look up the schema of the database tables
          - /refinesql: Evaluate and optimize SQL queries
          - /queryredshift: Execute SQL queries on the database
          -/queryPubMed: Query PubMed for public biomedical literature
          - /plot_kaplan_meier: Generate Kaplan-Meier survival charts
          - /fit_survival_regression: Fit a survival regression model with data in a S3 object
          - compute_imaging_biomarker: Trigger long-running job for CT lung imaging biomarker processing
          - analyze_imaging_biomarker: Analyze results of imaging biomarker computation
          -/bar_chart: Create a bar chart from the given input values.

        2. Before generating any SQL query, use the /getschema tool to familiarize yourself with the database structure. This will ensure your queries are correctly formatted and target the appropriate columns.

        3. When generating an SQL query:
          a. Write the query as a single line, removing all newline ("\n") characters.
          b. Always try to use aggregation and groupby when applicable.
          b. Before execution of a step, evaluate the SQL query with the rationale of the specific step by using the /refinesql tool. Provide both the SQL query and a brief rationale for the specific step you're taking. Do not share the original user question with the tool. 
          c. Only proceed to execute the query using the /queryredshift tool after receiving the evaluated and potentially optimized version from the /refinesql tool.

        4. When querying PubMed:
          a. Summarize the findings of each relevant study with citations to the specific pubmed web link of the study
          b. The json output will include  'Link', 'Title', 'Summary'.
          c. Always return the Title and Link (for example, 'https://pubmed.ncbi.nlm.nih.gov/') of each study in your response. 

        5. If the user query requires a Kaplan-Meier chart:
          a. Generate the necessary SQL query to retrieve the required data without any aggregation. 
          c. Map survival status as 0 for Alive and 1 for Dead for the event parameter.
          d. Use survival duration as the duration parameter.
          e. Use the /group_survival_data tool to create baseline and condition group based on expression value threshold provided by the user.

        6. If a survival regression analysis is needed:
          a. Retrieve all records with columns start with survival status as first column, then survival duration, and the required biomarkers.
          b. Use the /fit_survival_regression tool to identify the best-performing biomarker based on the p-value summary.

        7. For computed tomographic (CT) lung imaging biomarker analysis:
          a. Identify the patient subject ID(s) based on the conversation.
          b. Use the compute_imaging_biomarker tool to trigger the long-running job, passing the subject ID(s) as an array of strings (for example, ["R01-043", "R01-93"]).
          c. Only if specifically asked for an analysis, use the analyze_imaging_biomarker tool to process the results from the previous job.

        8. For literature evidence, make use of the knowledge base to retrieve relevant information.

        9. When you need to create a bar chart or plot:
          a. Always pass x_values and y_values in Array type to the function. If the user says x values are apple,egg and y values are 3,4 or as [apple,egg] and [3,4] pass their value as ['apple', 'banana'] and [3,4]
         
        10. When providing your response:
          a. Start with a brief summary of your understanding of the user's query.
          b. Explain the steps you're taking to address the query. Ask for clarifications from the user if required. 
          c. Present the results of your database queries and any additional analyses.
          d. If you generate any charts or perform statistical analyses, explain their significance in the context of the user's query.
          e. Conclude with a concise summary of the findings and their potential implications for medical research.

        Make sure to explain any medical or statistical concepts in a clear, accessible manner.



      Description: "Agent for querying biomaker database."
      ActionGroups:
        - ActionGroupName: 'UserInputAction'
          ParentActionGroupSignature: 'AMAZON.UserInput'
          ActionGroupState: 'ENABLED'
        - ActionGroupName: sqlActionGroup
          Description: Action for getting the database schema and querying the database
          ActionGroupExecutor: 
            Lambda: !GetAtt AgentLambdaFunction.Arn
          ApiSchema:
            Payload: |
              {
                "openapi": "3.0.1",
                "info": {
                  "title": "Database schema look up and query APIs",
                  "version": "1.0.0",
                  "description": "APIs for looking up database table schemas and making queries to database tables."
                },
                "paths": {
                  "/getschema": {
                    "get": {
                      "summary": "Get a list of all columns in the redshift database",
                      "description": "Get the list of all columns in the redshift database table. Return all the column information in database table.",
                      "operationId": "getschema",
                      "responses": {
                        "200": {
                          "description": "Gets the list of table names and their schemas in the database",
                          "content": {
                            "application/json": {
                              "schema": {
                                "type": "array",
                                "items": {
                                  "type": "object",
                                  "properties": {
                                    "Table": {
                                      "type": "string",
                                      "description": "The name of the table in the database."
                                    },
                                    "Schema": {
                                      "type": "string",
                                      "description": "The schema of the table in the database. Contains all columns needed for making queries."
                                    }
                                  }
                                }
                              }
                            }
                          }
                        }
                      }
                    }
                  },
                  "/queryredshift": {
                    "get": {
                      "summary": "API to send query to the redshift database table",
                      "description": "Send a query to the database table to retrieve information pertaining to the users question. The API takes in only one SQL query at a time, sends the SQL statement and returns the query results from the table. This API should be called for each SQL query to a database table.",
                      "operationId": "queryredshift",
                      "parameters": [
                        {
                          "name": "query",
                          "in": "query",
                          "required": true,
                          "schema": {
                            "type": "string"
                          },
                          "description": "SQL statement to query database table."
                        }
                      ],
                      "responses": {
                        "200": {
                          "description": "Query sent successfully",
                          "content": {
                            "application/json": {
                              "schema": {
                                "type": "object",
                                "properties": {
                                  "responseBody": {
                                    "type": "string",
                                    "description": "The query response from the database."
                                  }
                                }
                              }
                            }
                          }
                        },
                        "400": {
                          "description": "Bad request. One or more required fields are missing or invalid."
                        }
                      }
                    }
                  },
                  "/refinesql": {
                    "get": {
                      "summary": "Evaluate SQL query efficiency",
                      "description": "Evaluate the efficiency of an SQL query based on the provided schema, query, and question.",
                      "operationId": "refinesql",
                      "parameters": [
                        {
                          "name": "sql",
                          "in": "query",
                          "required": true,
                          "schema": {
                            "type": "string"
                          },
                          "description": "The SQL query to evaluate."
                        },
                        {
                          "name": "question",
                          "in": "query",
                          "required": true,
                          "schema": {
                            "type": "string"
                          },
                          "description": "The question related to the rationale of the specific step."
                        }
                      ],
                      "responses": {
                        "200": {
                          "description": "Successful response",
                          "content": {
                            "application/json": {
                              "schema": {
                                "type": "object",
                                "properties": {
                                  "evaluatedQuery": {
                                    "type": "string",
                                    "description": "The evaluated SQL query, or the original query if it is efficient."
                                  }
                                }
                              }
                            }
                          }
                        },
                        "400": {
                          "description": "Bad request. One or more required fields are missing or invalid."
                        }
                      }
                    }
                  }
                }
              }
        - ActionGroupName: scientificAnalysisActionGroup
          Description: Actions for scientific analysis with lifelines library
          ActionGroupExecutor: 
            Lambda: !GetAtt ScientificPlotLambdaFunction.Arn
          FunctionSchema:
            Functions:
              - Description: "Plots a Kaplan-Meier survival chart"
                Name: "plot_kaplan_meier"
                Parameters:
                  biomarker_name:
                    Type: "string"
                    Description: "name of the biomarker"
                    Required: true
                  duration_baseline:
                    Type: "array"
                    Description: "duration in number of days for baseline"
                    Required: true
                  duration_condition:
                    Type: "array"
                    Description: "duration in number of days for condition"
                    Required: true
                  event_baseline:
                    Type: "array"
                    Description: "survival event for baseline"
                    Required: true
                  event_condition:
                    Type: "array"
                    Description: "survival event for condition"
                    Required: true
              - Description: "Fit a survival regression model with data in a S3 object"
                Name: "fit_survival_regression"
                Parameters:
                  bucket:
                    Type: "string"
                    Description: "s3 bucket where the data is stored by the database query tool"
                    Required: true
                  key:
                    Type: "string"
                    Description: "json file name that is located in the s3 bucket and contains the data for fitting the model"
                    Required: true
        - ActionGroupName: imagingBiomarkerProcessing
          Description: Actions for processing imaging biomarker within CT scans for a list of subjects
          ActionGroupExecutor: 
            Lambda: !GetAtt ImagingBiomarkerLambda.Arn
          FunctionSchema:
            Functions:
              - Description: "compute the imaging biomarker features from lung CT scans within the tumor for a list of patient subject ID"
                Name: "compute_imaging_biomarker"
                Parameters:
                  subject_id:
                    Type: "array"
                    Description: "an array of patient subject ID"
                    Required: true
              - Description: "analyze the result imaging biomarker features from lung CT scans within the tumor for a list of patient subject ID"
                Name: "analyze_imaging_biomarker"
                Parameters:
                  subject_id:
                    Type: "array"
                    Description: "an array of patient subject ID"
                    Required: true
        - ActionGroupName: survivalDataProcessing
          Description: Process survival data of patients in order to invoke other tools
          ActionGroupExecutor: 
            Lambda: !GetAtt SurvivalDataProcessingLambdaFunction.Arn
          FunctionSchema:
            Functions:
              - Description: "Group based on threshold values"
                Name: "group_survival_data"
                Parameters:
                  biomarker:
                    Type: "array"
                    Description: "biomarker expression values, code input not accepted"
                    Required: true
                  survival_duration:
                    Type: "array"
                    Description: "survival duration values, code input not accepted"
                    Required: true
                  survival_status:
                    Type: "array"
                    Description: "survival status values, code input not accepted"
                    Required: true
                  threshold:
                    Type: "number"
                    Description: "Threshold value, code input not accepted"
                    Required: true

        - ActionGroupName: matplotbarchart
          Description: Creates a bar chart from the given input values
          ActionGroupExecutor: 
            Lambda: !GetAtt MatPlotBarChartLambdaFunction.Arn
          FunctionSchema:
            Functions:
              - Description: "create a bar chart"
                Name: "bar_chart"
                Parameters:
                  title:
                    Type: "string"
                    Description: "title of the bar chart"
                    Required: true
                  x_values:
                    Type: "array"
                    Description: "values for the x a xis"
                    Required: true
                  y_values:
                    Type: "array"
                    Description: "values for the y axis"
                    Required: true
                  x_label:
                    Type: "string"
                    Description: "title of the x axis"
                    Required: true
                  y_label:
                    Type: "string"
                    Description: "title of the y axis"
                    Required: true

        - ActionGroupName: queryPubMed
          Description: Actions for fetching biomedical literature from PubMed
          ActionGroupExecutor: 
            Lambda: !GetAtt QueryPubMedLambdaFunction.Arn
          ApiSchema:
            Payload: |
                {
                    "openapi": "3.0.0",
                    "info": {
                        "title": "fetch biomedical literature",
                        "version": "1.0.0",
                        "description": "PubMed API to help answer users question using abstracts from biomedical literature."
                    },
                    "paths": {
                        "/query-pubmed": {
                            "post": {
                                "summary": "Query pubmed to relevant information from abstracts of biomedical articles.",
                                "description": "Query pubmed to relevant information from abstracts of biomedical articles. The PubMed API takes in the user query then returns the abstracts of top 5 relevant articles.",
                                "operationId": "query-pubmed",
                                "x-requireConfirmation": "ENABLED"
                                "parameters": [
                                    {
                                        "name": "query",
                                        "in": "query",
                                        "description": "user query",
                                        "required": true,
                                        "schema": {
                                            "type": "string"
                                        }
                                    }
                                ],                
                                "responses": {
                                    "200": {
                                        "description": "Query pubmed to relevant information from abstracts of biomedical articles.",
                                        "content": {
                                            "application/json": {
                                                "schema": {
                                                    "type": "object",
                                                    "properties": {
                                                        "answer": {
                                                            "type": "string",
                                                            "description": "The response to user query with list of pubmed article abstracts."
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
      KnowledgeBases:
          - KnowledgeBaseId: !Ref BedrockKnowledgeBase
            Description: Literature evidence on Relationships between Molecular and Imaging Phenotypes with Prognostic Implications


      
  
  AgentAliasResource:
    Type: AWS::Bedrock::AgentAlias
    Properties:
      AgentId: !GetAtt AgentResource.AgentId
      AgentAliasName: !Sub biomakers-alias
      

  AgentRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub AmazonBedrockExecutionRoleForAgents_${AWS::AccountId}
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - bedrock.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
        - PolicyName: BedrockInvokeModel
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - bedrock:InvokeModel*
                  - bedrock:Retrieve
                Resource: 
                  - !Sub arn:aws:bedrock:us-east-1::foundation-model/${BedrockModelId}
                  - !Sub arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:knowledge-base/${BedrockKnowledgeBase}
                  - !Sub arn:aws:bedrock:us-west-2::foundation-model/${BedrockModelId}
                  - !Sub arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0
                  - !Sub arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0
                  - !Sub arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:inference-profile/us.anthropic.claude-3-5-sonnet-20240620-v1:0
                  - !Sub arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:inference-profile/us.anthropic.claude-3-sonnet-20240229-v1:0

      
  ########################
  ##### ActionGroup #####
  ######################

  AgentLambdaFunction:
    Type: AWS::Lambda::Function
    DependsOn: TriggerDataRetrievalLogic
    Properties:
      Runtime: python3.12
      FunctionName: !Sub biomarker-agent-${EnvironmentName}
      Handler: querydatabaselambda.lambda_handler
      Role: !GetAtt AgentLambdaRole.Arn
      Timeout: 900
      Environment:
        Variables:
          BUCKET_NAME: !Ref S3Bucket
      Code:
        S3Bucket: !Ref S3Bucket
        S3Key: querydatabaselambda.zip
      

  AgentLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
        - PolicyName: AgentLambdaPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Sid: LambdaExecution
                Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*
              - Sid: RedshiftDataAccess
                Effect: Allow
                Action:
                  - redshift-data:ExecuteStatement
                  - redshift-data:DescribeStatement
                  - redshift-data:GetStatementResult
                  - redshift-data:ListStatements
                Resource: '*'
              - Sid: RedshiftCredentials
                Effect: Allow
                Action:
                  - redshift:GetClusterCredentials
                Resource: 
                  - !Sub arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:dbuser:biomarker-redshift-cluster/admin
                  - !Sub arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:dbname:biomarker-redshift-cluster/dev
              - Sid: S3Access
                Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                Resource: 
                  - !Sub arn:aws:s3:::${S3Bucket}/*
              - Sid: BedrockAccess
                Effect: Allow
                Action:
                  - bedrock:InvokeModel
                Resource: !Sub arn:aws:bedrock:${AWS::Region}::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0
                    
                  
  AgentLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !GetAtt AgentLambdaFunction.Arn
      Principal: bedrock.amazonaws.com
      SourceArn: !GetAtt AgentResource.AgentArn

  ScientificPlotLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/AmazonBedrockFullAccess
      Policies:
        - PolicyName: S3ObjectPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                Resource: !Sub 'arn:aws:s3:::${S3Bucket}/*'

  ScientificPlotLambdaFunction:
    Type: AWS::Lambda::Function
    DependsOn: EnsureECRImagePushed
    Properties:
      FunctionName: ScientificPlotLambda
      PackageType: Image
      Code:
        ImageUri: !Sub ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ECRRepository}:${ImageTag}
      Role: !GetAtt ScientificPlotLambdaExecutionRole.Arn
      MemorySize: 512
      Timeout: 900
      Environment:
        Variables:
          S3_BUCKET: !Ref S3Bucket
  
  ScientificPlotLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !GetAtt ScientificPlotLambdaFunction.Arn
      Principal: bedrock.amazonaws.com
      SourceArn: !GetAtt AgentResource.AgentArn
  
  
  QueryPubMedLambdaRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
        - arn:aws:iam::aws:policy/AmazonBedrockFullAccess

  QueryPubMedLambdaFunction:
    Type: 'AWS::Lambda::Function'
    DependsOn: ScientificPlotLambdaFunction
    Properties:
      FunctionName: PubMedQueryFunction
      Handler: lambda_function.lambda_handler
      Role: !GetAtt QueryPubMedLambdaRole.Arn
      Code:
        S3Bucket: !Ref S3Bucket
        S3Key: pubmed-lambda-function.zip
      Runtime: python3.11
      Timeout: 30
      MemorySize: 128

  QueryPubMedLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !GetAtt QueryPubMedLambdaFunction.Arn
      Principal: bedrock.amazonaws.com
      SourceArn: !GetAtt AgentResource.AgentArn
  
  SurvivalDataProcessingLambdaRole:
    Type: 'AWS::IAM::Role'
    DependsOn: TriggerDataRetrievalLogic
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
        - arn:aws:iam::aws:policy/AmazonBedrockFullAccess

  SurvivalDataProcessingLambdaFunction:
    Type: 'AWS::Lambda::Function'
    Properties:
      FunctionName: SurvivalDataProcessingFunction
      Handler: survivaldataprocessinglambda.lambda_handler
      Role: !GetAtt SurvivalDataProcessingLambdaRole.Arn
      Code:
        S3Bucket: !Ref S3Bucket
        S3Key: survivaldataprocessinglambda.zip
      Runtime: python3.12
      Timeout: 30
      MemorySize: 128

  SurvivalDataProcessingLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !GetAtt SurvivalDataProcessingLambdaFunction.Arn
      Principal: bedrock.amazonaws.com
      SourceArn: !GetAtt AgentResource.AgentArn
  
  ImagingBiomarkerLambda:
    Type: AWS::Lambda::Function
    DependsOn: EnsureECRImagePushed
    Properties:
      FunctionName: imaging-biomarker-lambda
      Handler: dummy_lambda.lambda_handler
      Role: !GetAtt ImagingLambdaExecutionRole.Arn
      Code:
        S3Bucket: !Ref S3Bucket 
        S3Key: Imaginglambdafunction.zip  
      Runtime: python3.12
      Timeout: 300
      MemorySize: 256
      Environment:
        Variables:
          SFN_STATEMACHINE_NAME: !GetAtt ImagingStateMachine.Name
          REGION: !Sub ${AWS::Region}
          ACCOUNTID: !Sub ${AWS::AccountId}
          S3BUCKET: !Sub s3://${S3Bucket}
      Layers:
        - !FindInMap [RegionMap, !Ref 'AWS::Region', PandasLayer]
  
  ImagingLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: ImagingBiomarkerLambdaExecutionRole-${AWS::Region}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: ImagingLambdaExecutionPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: arn:aws:logs:*:*:*
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:ListBucket
                Resource:
                  - !Sub arn:aws:s3:::${S3Bucket}
                  - !Sub arn:aws:s3:::${S3Bucket}/*
              - Effect: Allow
                Action:
                  - states:StartExecution
                Resource: !Ref ImagingStateMachine
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
        - arn:aws:iam::aws:policy/AmazonBedrockFullAccess
  
  ImagingLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !GetAtt ImagingBiomarkerLambda.Arn
      Principal: bedrock.amazonaws.com
      SourceArn: !GetAtt AgentResource.AgentArn

  #------Add New Actions Here -----#

  MatPlotBarChartLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: MatPlotBarChartLambdaPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                  - s3:ListBucket
                Resource: 
                - !Sub 'arn:aws:s3:::${S3Bucket}'
                - !Sub 'arn:aws:s3:::${S3Bucket}/*'

  MatplotlibLayer:
    Type: AWS::Lambda::LayerVersion
    DependsOn: ScientificPlotLambdaFunction
    Properties:
      LayerName: matplotliblayer
      Description: Layer containing matplotlib and its dependencies
      Content:
        S3Bucket: !Ref S3Bucket
        S3Key: lambda-layer.zip
      CompatibleRuntimes:
        - python3.11
    

  MatPlotBarChartLambdaFunction:
    Type: AWS::Lambda::Function
    DependsOn: ImagingBiomarkerLambda
    Properties:
      FunctionName: MatPlotBarChartLambda
      Handler: matplotbarchartlambda.handler  
      Role: !GetAtt MatPlotBarChartLambdaExecutionRole.Arn  
      Code:
        S3Bucket: !Ref S3Bucket 
        S3Key: matplotbarchartlambda.zip  
      Runtime: python3.11  
      Timeout: 500
      MemorySize: 128
      Layers:
        - !Ref MatplotlibLayer
      Environment:
        Variables:
          S3_BUCKET: !Ref S3Bucket


  MatPlotBarChartLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !GetAtt MatPlotBarChartLambdaFunction.Arn
      Principal: bedrock.amazonaws.com
      SourceArn: !GetAtt AgentResource.AgentArn


  AgentIdSSMParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Sub /streamlitapp/${EnvironmentName}/AGENT_ID
      Type: String
      Value: !GetAtt AgentResource.AgentId
      Description: !Sub SSM parameter for AgentId for ${EnvironmentName}

  AgentAliasIdSSMParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Sub /streamlitapp/${EnvironmentName}/AGENT_ALIAS_ID
      Type: String
      Value: !GetAtt AgentAliasResource.AgentAliasId
      Description: !Sub SSM parameter for AgentAliasId for ${EnvironmentName}


  S3BucketNameSSMParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Sub /streamlitapp/${EnvironmentName}/S3_BUCKET_NAME
      Type: String
      Value: !Ref S3Bucket
      Description: !Sub SSM parameter for S3 bucket name for ${EnvironmentName}

  
  # enableCodeInterpreter:
  #   Type: Custom::enableCodeInterpreter
  #   DependsOn: 
  #     - AgentAliasResource
  #     - AgentAliasIdSSMParameter
  #   Properties:
  #     ServiceToken: !GetAtt CodeInterpreterLambda.Arn
  #     agentId: !GetAtt AgentResource.AgentId
  #     agentVersion: !GetAtt AgentResource.AgentVersion

  # CodeInterpreterLambda:
  #   Type: AWS::Lambda::Function
  #   Properties:
  #     Handler: index.handler
  #     Role: !GetAtt CodeInterpreterLambdaRole.Arn
  #     Code:
  #       ZipFile: |
  #         import boto3
  #         import cfnresponse
  #         import os
  #         import time

  #         def create_ssm_parameter(environment_name, agent_alias_id):
  #             ssm_client = boto3.client('ssm')
  #             parameter_name = f"/streamlitapp/{environment_name}/AGENT_ALIAS_ID"
  #             parameter_value = agent_alias_id
  #             parameter_type = "String"
  #             parameter_description = f"SSM parameter for AgentAliasId for {environment_name}"

  #             try:
  #                 ssm_client.put_parameter(
  #                     Name=parameter_name,
  #                     Value=parameter_value,
  #                     Type=parameter_type,
  #                     Description=parameter_description,
  #                     Overwrite=True
  #                 )
  #                 print(f"SSM Parameter '{parameter_name}' created/updated successfully.")
  #             except Exception as e:
  #                 print(f"Error creating/updating SSM Parameter: {str(e)}")
  #                 raise

  #         def get_ssm_parameter(environment_name):
  #             ssm_client = boto3.client('ssm')
  #             parameter_name = f"/streamlitapp/{environment_name}/AGENT_ALIAS_ID"
              
  #             try:
  #                 response = ssm_client.get_parameter(Name=parameter_name)
  #                 return response['Parameter']['Value']
  #             except ssm_client.exceptions.ParameterNotFound:
  #                 print(f"SSM Parameter '{parameter_name}' not found.")
  #                 return None
  #             except Exception as e:
  #                 print(f"Error retrieving SSM Parameter: {str(e)}")
  #                 raise

  #         def prepare_agent(client, agent_id):
  #             MAX_ATTEMPTS = 30
  #             DELAY_SECONDS = 10
  #             agentPrep = client.prepare_agent(agentId=agent_id)

  #             for attempt in range(MAX_ATTEMPTS):
  #                 print(f"Agent status is: {agentPrep['agentStatus']} (Attempt {attempt + 1}/{MAX_ATTEMPTS})")
  #                 response = client.get_agent(agentId=agent_id)
  #                 agentStatus=response['agent']['agentStatus']
                  
  #                 if agentStatus == 'PREPARED':
  #                     return True
  #                 elif agentStatus == 'FAILED':
  #                     return False
                  
  #                 if attempt < MAX_ATTEMPTS - 1:
  #                     time.sleep(DELAY_SECONDS)

  #             return False

  #         def handler(event, context):
  #             client = boto3.client('bedrock-agent')
  #             environment_name = os.environ['ENVIRONMENT_NAME']
              
  #             try:
  #                 if event['RequestType'] in ['Create', 'Update']:
  #                     response = client.create_agent_action_group(
  #                         actionGroupName='CodeInterpreterAction',
  #                         actionGroupState='ENABLED',
  #                         agentId=os.environ['agentId'],
  #                         agentVersion=os.environ['agentVersion'],
  #                         parentActionGroupSignature='AMAZON.CodeInterpreter'
  #                     )
  #                     agent_id = os.environ['agentId']
                      
  #                     if not prepare_agent(client, agent_id):
  #                         raise Exception("Agent failed to prepare or preparation timed out")

  #                     agentAlias = client.create_agent_alias(agentAliasName='v2', agentId=agent_id)
  #                     agent_alias_id = agentAlias['agentAlias']['agentAliasId']
                  
  #                     # Create SSM parameter
  #                     create_ssm_parameter(environment_name, agent_alias_id)
                      
  #                     cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Message": "Action group and SSM parameter created successfully"})
  #                 elif event['RequestType'] == 'Delete':
  #                     # Retrieve the agent alias ID from SSM
  #                     agent_alias_id = get_ssm_parameter(environment_name)
                      
  #                     if agent_alias_id:
  #                         # Delete the agent alias
  #                         response = client.delete_agent_alias(agentAliasId=agent_alias_id, agentId=os.environ['agentId'])
  #                         print(f"Agent alias {agent_alias_id} deleted successfully.")
                          
  #                         cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Message": "Agent alias and SSM parameter deleted successfully"})
  #                     else:
  #                         cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Message": "No agent alias found to delete"})
  #                 else:
  #                     cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": "Unsupported request type"})
  #             except Exception as e:
  #                 print(f"Error: {str(e)}")
  #                 cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": str(e)})

  #     Runtime: python3.12
  #     Timeout: 300
  #     Environment:
  #       Variables:
  #         agentId: !GetAtt AgentResource.AgentId
  #         agentVersion: !GetAtt AgentResource.AgentVersion
  #         ENVIRONMENT_NAME: !Ref EnvironmentName

  # CodeInterpreterLambdaRole:
  #   Type: AWS::IAM::Role
  #   Properties:
  #     AssumeRolePolicyDocument:
  #       Version: '2012-10-17'
  #       Statement:
  #         - Effect: Allow
  #           Principal:
  #             Service:
  #               - lambda.amazonaws.com
  #           Action:
  #             - sts:AssumeRole
  #     Policies:
  #       - PolicyName: CloudWatchLogsPolicy
  #         PolicyDocument:
  #           Version: '2012-10-17'
  #           Statement:
  #             - Effect: Allow
  #               Action:
  #                 - logs:CreateLogGroup
  #                 - logs:CreateLogStream
  #                 - logs:PutLogEvents
  #               Resource: !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*
  #       - PolicyName: BedrockAgentPolicy
  #         PolicyDocument:
  #           Version: '2012-10-17'
  #           Statement:
  #             - Effect: Allow
  #               Action:
  #                 - bedrock:CreateAgentActionGroup
  #                 - bedrock:PrepareAgent
  #                 - bedrock:CreateAgentAlias
  #                 - bedrock:DeleteAgentAlias
  #                 - bedrock:GetAgent
  #               Resource: !Sub arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:agent/${AgentResource}
  #       - PolicyName: BedrockAgentAliasPolicy
  #         PolicyDocument:
  #           Version: '2012-10-17'
  #           Statement:
  #             - Effect: Allow
  #               Action:
  #                 - bedrock:CreateAgentAlias
  #                 - bedrock:DeleteAgentAlias
  #               Resource: !Sub arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:agent-alias/*
  #       - PolicyName: SSMParameterPolicy
  #         PolicyDocument:
  #           Version: '2012-10-17'
  #           Statement:
  #             - Effect: Allow
  #               Action:
  #                 - ssm:GetParameter
  #                 - ssm:PutParameter
  #               Resource: !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/streamlitapp/*
  BedrockKBExecutionRole:
    Type: 'AWS::IAM::Role'
    Properties:
      RoleName: !Sub 'AmazonBedrockExecutionRoleForKnowledgeBase_${EnvironmentName}_${AWS::Region}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: 
                - bedrock.amazonaws.com
            Action: sts:AssumeRole
            Condition:
              StringEquals:
                aws:SourceAccount: !Ref 'AWS::AccountId'
              ArnLike:
                aws:SourceArn: !Sub 'arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:knowledge-base/*'
      Policies:
      - PolicyName: FoundationModelPolicy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - bedrock:InvokeModel
            Resource: 
              - !Sub 'arn:aws:bedrock:${AWS::Region}::foundation-model/amazon.titan-embed-text-v1'
              - !Sub 'arn:aws:bedrock:${AWS::Region}::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0'
              - !Sub 'arn:aws:bedrock:${AWS::Region}::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0'
            Sid: BedrockInvokeModelStatement
      - PolicyName: OSSPolicy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - aoss:APIAccessAll
            Resource: !Sub 'arn:aws:aoss:${AWS::Region}:${AWS::AccountId}:collection/${Collection}'
            Sid: OpenSearchServerlessAPIAccessAllStatement
      - PolicyName: S3Policy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - s3:ListBucket
            Resource: !Sub 'arn:aws:s3:::${S3Bucket}'
            Sid: S3ListBucketStatement
          - Effect: Allow
            Action:
            - s3:GetObject
            Resource: !Sub 'arn:aws:s3:::${S3Bucket}/*'
            Sid: S3GetObjectStatement

  EncryptionPolicy:
    Type: 'AWS::OpenSearchServerless::SecurityPolicy'
    Properties:
      Name: !Sub '${EnvironmentName}-encryption-policy'
      Type: encryption
      Description: !Sub 'Encryption policy for ${AWS::StackName} collection'
      Policy: !Sub |
        {
          "Rules": [
            {
              "ResourceType": "collection",
              "Resource": ["collection/${EnvironmentName}-${VectorStoreName}"]
            }
          ],
          "AWSOwnedKey": true
        }

  NetworkPolicy:
    Type: 'AWS::OpenSearchServerless::SecurityPolicy'
    Properties:
      Name: !Sub '${EnvironmentName}-network-policy'
      Type: network
      Description: !Sub 'Network policy for ${AWS::StackName} collection'
      Policy: !Sub |
        [
          {
            "Rules": [
              {
                "ResourceType": "collection",
                "Resource": ["collection/${EnvironmentName}-${VectorStoreName}"]
              }
            ],
            "AllowFromPublic": true
          }
        ]

  Collection:
    Type: 'AWS::OpenSearchServerless::Collection'
    Properties:
      Name: !Sub '${EnvironmentName}-${VectorStoreName}'
      Type: VECTORSEARCH
      Description: !Sub 'Collection to hold vector for ${AWS::StackName}'
    DependsOn: 
      - EncryptionPolicy
      - NetworkPolicy

  OpenSearchIndexLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: LambdaBasicExecution
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: 'arn:aws:logs:*:*:*'
        - PolicyName: allowAoss
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
            - Effect: Allow
              Action:
              - aoss:APIAccessAll
              - aoss:List*
              - aoss:Get*
              - aoss:Create*
              - aoss:Update*
              - aoss:Delete*
              Resource: 'arn:aws:logs:*:*:*'
        - PolicyName: OSSLambdaRoleDefaultPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
            - Effect: Allow
              Action:
              - xray:PutTelemetryRecords
              - xray:PutTraceSegments
              Resource: '*'

  OpenSearchIndexLambda:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt OpenSearchIndexLambdaRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import json
          import time
          from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth

          def create_index_with_retry(client, index_name, index_body, max_retries=5, base_delay=5):
              for attempt in range(max_retries):
                  try:
                      response = client.indices.create(index=index_name, body=json.dumps(index_body))
                      print(f"Index created: {response}")
                      return True
                  except Exception as e:
                      print(f"Attempt {attempt + 1} failed: {str(e)}")
                      if attempt < max_retries - 1:
                          delay = base_delay * (2 ** attempt)  # Exponential backoff
                          print(f"Retrying in {delay} seconds...")
                          time.sleep(delay)
                      else:
                          print("Max retries reached. Index creation failed.")
                          return False

          def handler(event, context):
              if event['RequestType'] in ['Create', 'Update']:
                  try:
                      collection_name = event['ResourceProperties']['CollectionName']
                      index_name = event['ResourceProperties']['IndexName']
                      collection_id = event["ResourceProperties"]["CollectionId"]
                      region = event["ResourceProperties"]["Region"]
                      
                      print(f"Collection Name: {collection_name}")
                      print(f"Index Name: {index_name}")
                      print(f"Collection ID: {collection_id}")
                      print(f"Region: {region}")
                      
                      service = 'aoss'
                      host = f"{collection_id}.{region}.{service}.amazonaws.com"
                      credentials = boto3.Session().get_credentials()
                      awsauth = AWSV4SignerAuth(credentials, region, service)
                      
                      client = OpenSearch(
                          hosts=[{'host': host, 'port': 443}],
                          http_auth=awsauth,
                          use_ssl=True,
                          verify_certs=True,
                          connection_class=RequestsHttpConnection,
                          pool_maxsize=20,
                      )
                      
                      # Updated index_body to match Amazon Titan model specifications
                      index_body = {
                          "settings": {
                              "index": {
                                  "knn": True,
                                  "knn.algo_param.ef_search": 512
                              }
                          },
                          "mappings": {
                              "properties": {
                                  "bedrock-knowledge-base-default-vector": {
                                      "type": "knn_vector",
                                      "dimension": 1536,
                                      "method": {
                                          "name": "hnsw",
                                          "engine": "faiss",
                                          "parameters": {
                                              "ef_construction": 512,
                                              "m": 16
                                          },
                                          "space_type": "l2",
                                      },
                                  },
                                  "AMAZON_BEDROCK_METADATA": {
                                      "type": "text",
                                      "index": "false"
                                  },
                                  "AMAZON_BEDROCK_TEXT_CHUNK": {
                                      "type": "text",
                                      "index": "true"
                                  },
                              }
                          }
                      }
                      
                      # Initial delay before attempting to create the index
                      print("Waiting 50 seconds before attempting to create the index...")
                      time.sleep(50)
                      
                      # Attempt to create the index with retry logic
                      if create_index_with_retry(client, index_name, index_body):
                          print("Waiting 60 seconds for the index to be fully created...")
                          time.sleep(60)
                          cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                      else:
                          raise Exception("Failed to create index after multiple attempts")
                  
                  except Exception as e:
                      print(f"Error: {str(e)}")
                      cfnresponse.send(event, context, cfnresponse.FAILED, {})
              else:
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})

      Runtime: python3.12
      Timeout: 900
      Layers: 
        - !FindInMap [RegionMap, !Ref 'AWS::Region', PandasLayer]

  CreateOpenSearchIndex:
    Type: Custom::CreateOpenSearchIndex
    Properties:
      ServiceToken: !GetAtt OpenSearchIndexLambda.Arn
      CollectionName: !Sub '${AWS::StackName}-${VectorStoreName}'
      IndexName: !Ref IndexName
      CollectionId: !GetAtt Collection.Id
      Region: !Ref 'AWS::Region'
    DependsOn:
      - Collection
      - BedrockOSSPolicyForKB

  BedrockOSSPolicyForKB:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      ManagedPolicyName: !Sub ${AWS::StackName}-bedrock-oss-policy
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: OpenSearchServerlessAPIAccessAllStatement
            Effect: Allow
            Action:
              - 'aoss:APIAccessAll'
            Resource: !GetAtt Collection.Arn
      Roles:
        - !Ref BedrockKBExecutionRole
        - !Ref OpenSearchIndexLambdaRole

  DataAccessPolicy:
    Type: 'AWS::OpenSearchServerless::AccessPolicy'
    Properties:
      Name: !Sub '${EnvironmentName}-access-policy'
            
      Type: data
      Description: !Sub 'Access policy for ${AWS::StackName} collection'
      Policy: !Sub |
        [
          {
            "Rules": [
              {
                "ResourceType": "collection",
                "Resource": ["collection/${EnvironmentName}-${VectorStoreName}"],
                "Permission": [
                  "aoss:CreateCollectionItems",
                  "aoss:DeleteCollectionItems",
                  "aoss:UpdateCollectionItems",
                  "aoss:DescribeCollectionItems"
                ]
              },
              {
                "ResourceType": "index",
                "Resource": ["index/${EnvironmentName}-${VectorStoreName}/*"],
                "Permission": [
                  "aoss:CreateIndex",
                  "aoss:DeleteIndex",
                  "aoss:UpdateIndex",
                  "aoss:DescribeIndex",
                  "aoss:ReadDocument",
                  "aoss:WriteDocument"
                ]
              }
            ],
            "Principal": [
              "${BedrockKBExecutionRole.Arn}",
              "arn:aws:iam::${AWS::AccountId}:role/aws-service-role/bedrock.amazonaws.com/AWSServiceRoleForAmazonBedrock",
              "${OpenSearchIndexLambdaRole.Arn}"
            ],
            "Description": "Data access policy for OpenSearch and Bedrock"
          }
        ]
    DependsOn:
      - Collection
      - BedrockKBExecutionRole
      - OpenSearchIndexLambdaRole

  BedrockKnowledgeBase:
    Type: AWS::Bedrock::KnowledgeBase
    Properties:
      Name: !Sub '${AWS::StackName}-ncbiKnowledgebase'
      Description: !Sub 'A knowledge base created using CloudFormation for ${AWS::StackName}'
      KnowledgeBaseConfiguration:
        Type: VECTOR
        VectorKnowledgeBaseConfiguration:
          EmbeddingModelArn: !Sub 'arn:aws:bedrock:${AWS::Region}::foundation-model/amazon.titan-embed-text-v1'
      RoleArn: !GetAtt BedrockKBExecutionRole.Arn
      StorageConfiguration:
        Type: OPENSEARCH_SERVERLESS
        OpensearchServerlessConfiguration:
          CollectionArn: !GetAtt Collection.Arn
          FieldMapping: 
              VectorField: "bedrock-knowledge-base-default-vector"
              TextField: "text"
              MetadataField: "metadata"
          VectorIndexName: !Ref IndexName
    DependsOn: 
      - DataAccessPolicy
      - CreateOpenSearchIndex

  KnowledgeBaseDataSource:
    Type: AWS::Bedrock::DataSource
    DependsOn:
      - BedrockKnowledgeBase
      - DataRetrievalLogic
    Properties:
      DataSourceConfiguration:
        Type: S3
        S3Configuration:
          BucketArn: !GetAtt S3Bucket.Arn
          InclusionPrefixes:
            - "ncbi_article.pdf"
      Description: !Sub 'Knowledge base Data Source for ${AWS::StackName} (NCBI article)'
      KnowledgeBaseId: !Ref BedrockKnowledgeBase
      Name: !Sub '${AWS::StackName}-ncbiKnowledgebaseds'
      VectorIngestionConfiguration:
        ChunkingConfiguration:
          ChunkingStrategy: FIXED_SIZE
          FixedSizeChunkingConfiguration:
            MaxTokens: 300
            OverlapPercentage: 20
